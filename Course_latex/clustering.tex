\section{Clustering problems}

Given a set $X$ of points, a metric $d$ over $X$ and an integer $k \geq 1$.
We want to find a set $C \in \binom{X}{k}$, of "centers", s.t. some function is minimized.

Depending on the kind of function that we want to minimize, we have a different clustering problem.
We are now going to see some of the most important ones.

\textbf{K-Means} is defined over an $\ell_2$-loss. The function to minimize is:
\[ \sum_{x \in X} \min_{c \in C} d^2(x,c) \]

\textbf{K-Median} is defined over an $\ell_1$-loss. The function to minimize is:
\[ \sum_{x \in X} \min_{c \in C} d(x,c) \]

\textbf{K-Center} is defined over an $\ell_\infty$-loss. The function to minimize is:
\[ \max_{x \in X} \min_{c \in C} d(x,c) \]

In general, one can define the clustering with any $\ell_p$-loss.

K-Means is often defined as \href{https://en.wikipedia.org/wiki/Lloyd%27s_algorithm}{Lloyd's algorithm}, which is an error.
Lloyd's algorithm has been proposed as an heuristic for solving K-Means. It is quite simple to understand, but in general it can have a bad approximation, and a bad running time.

\subsection{K-Center}
    We are going to study K-Center, which, among the three clustering problems defined above, is the one with the simplest approximation.
    In Algorithm~\ref{alg:greedy_k_center} a greedy approximation for K-Center.

    Recall that, for a point $x$, and a set of points $S$, we have defined
    \[ d(x,S) = \min_{s \in S} d(x,s) \]

    \input{algorithms/greedy_k_center.tex}

    \begin{theorem}
        \textsc{Greedy} returns a $2$-approximation for K-Center.
    \end{theorem}

    \begin{proof}
        The algorithm defines sets $C_1, \dots, C_k$ and $x_1, \dots, x_k$, s.t. for each $i$, $C_i = \{ x_1, \dots, x_i \}$.
        It also defines the distances $\Delta_1, \dots, \Delta_{k-1}$.

        Let us define:
        \begin{equation*}
            \begin{split}
                &\Delta_k = \max_{x \in X} d(x, C_k)\\
                &x_{k+1} \in \argmax_{x \in X} d(x, C_k)\\
                &C_{k+1} = C_k \cup \{ x_{k+1} \}
            \end{split}
        \end{equation*}
        Note that we are simulating an extra iteration of the for loop.

        \begin{claim}\label{claim:kcenter_1}
            $C_k$, the set returned by \textsc{Greedy}, acts as a K-Center solution of value $\Delta_k$
        \end{claim}
        \begin{proof}
            The value of $C_k$ is
            \[ C_k = \max_{x \in X} \min_{y \in C_k} d(x,y) = \max_{x \in X} d(x, C_k) = \Delta_k \]
        \end{proof}

        \begin{claim}\label{claim:kcenter_2}
            $\Delta_1 \geq \Delta_2 \geq \cdots \geq \Delta_k$ (monotone decreasing)
        \end{claim}
        \begin{proof}
            Suppose $S \subseteq T \subseteq X$, and $x \in X$.
            Then
            \[ d(x,S) = \min_{y \in S} d(x,y) \overset{S \subseteq T}{\geq} \min_{y \in T} d(x,y) = d(x,T) \]

            For each $i = 2, \dots, k$, it holds that
            \[ \Delta_{i-1} = d(x_i, C_{i-1}) \overset{\text{greedy}}{\geq} d(x_{i+1}, C_{i-1}) \overset{C_{i-1} \subseteq C_i}{\geq} d(x_{i+1}, C_i) = \Delta_i \]
        \end{proof}

        \begin{claim}\label{claim:kcenter_3}
            $\forall i = 2, \dots, k+1$, $\forall \{ x_j, x_{j'} \} \in \binom{C_i}{2}$, $d(x_j, x_{j'}) \geq \Delta_{i-1}$.
        \end{claim}
        \begin{proof}
            By induction on $i$.

            Base case ($i=2$).
            $x_2$ is at distance $\Delta_1$ from $x_1$, and thus from $C_1 = \{x_1\}$.

            Inductive case. Suppose the claim holds for $i \leq k$; we prove it for $i+1$.
            Since the claim holds at $i$, we have $\forall \{ x_j, x_{j'} \} \in \binom{C_i}{2}$, $d(x_j, x_{j'}) \geq \Delta_{i-1}$.

            The generic pair $\{ x_j, x_{j'} \} \in \binom{C_{i+1}}{2}$ either contains two points from $C_i$, or it contains $x_{i+1}$ and one point from $C_i$.
            Since $\Delta_{i-1} \geq \Delta_i$, w.m.a. that one point is $x_{i+1}$. Wlog, $x_{j'} = x_{i+1}$.
            We have $\Delta_i = \max_{x \in X} d(x, C_i) = d(x_{i+1}, C_i) = \min_{y \in C_i} d(x_{i+1}, y)$.

            Thus, $\forall x_j \in C_i$, $d(x_{i+1}, x_j) \geq \Delta_i$.
            Then, $\forall \{ x_j, x_{j'} \} \subseteq C_{i+1}$ s.t. $x_{i+1} \in \{ x_j, x_{j'} \}$, $d( x_j, x_{j'} ) \geq \Delta_i$.
        \end{proof}

        \begin{claim}\label{claim:kcenter_4}
            If $C$ is any solution, that is, $C \in \binom{X}{k}$, then
            \[ \max_{x \in X} \min_{y \in C} d(x,y) \geq \frac{\Delta_k}{2} \]
        \end{claim}
        \begin{proof}
            Let $V = \max_{x \in X} \min_{y \in C} d(x,y)$ be the value of the solution $C$.
            Then, $\forall x \in X$, $\exists y = y_x \in C$ s.t. $d(x, y_x) \leq V$.

            Consider the set $C_{k+1} = C_k \cup \{ x_{k+1} \}$, then $\cardinality{C_{k+1}} = k+1$.
            By the PigeonHole Principle (PHP), $\exists \{ x_j, x_{j'} \} \in \binom{C_{k+1}}{2}$ s.t. $y_{x_j} = y_{x_{j'}}$.

            Let $x_j, x_{j'}$ be two points of $C_{k+1}$, $x_j \neq x_{j'}$, that have the same $y \overset{\Delta}{=} y_{x_j} = y_{x_{j'}} \in C$.

            Then, $d(x_j, y) \leq V$, and $d(x_{j'}, y) \leq V$.
            By the triangle inequality $d(x_j, x_{j'}) \leq d(x_j, y) + d(y, x_{j'}) \leq 2V$.

            By Claim~\ref{claim:kcenter_3}, any two distinct points from $C_{k+1}$ are at distance at least $\Delta_k$ from one another.
            Thurs,
            \[ \Delta_k \leq d(x_j, x_{j'}) \leq 2V \rightarrow V \geq \frac{\Delta_k}{2} \]
        \end{proof}

        Claims~\ref{claim:kcenter_1} and~\ref{claim:kcenter_4} entail that \textsc{Greedy} returns a $2$-approximation.
    \end{proof}

    \begin{theorem}
        $\forall \varepsilon > 0$, K-Center is NP-Hard to approximate to $2 - \varepsilon$.
    \end{theorem}


\subsection{Dominating set}
    Input: undirected graph $G(V,E)$, $k \geq 1$

    Question: does there exist $S \in \binom{V}{k}$ s.t. $\forall v \in V$, $v \in S$ or $\exists w \in S$ s.t. $\{ v,w \} \in E$

    That is, a dominating set is one that either contains all the vertices, or it touches all the edges.

    \begin{theorem}
        The Dominating set is NP-Hard
    \end{theorem}

    Let us define the cost of a K-Center solution $S$ as
    \[ Cen(S) = \max_{v \in V} \min_{c \in S} d(v,c) \]

    \begin{theorem}
        It is NP-Hard to approximate K-Center to $\alpha$, $\forall \alpha < 2$.
    \end{theorem}

    \begin{proof}
        Let $G(V,E),k$ be an instance of Dominating Set.

        We construct a metric $d : V \times V \rightarrow \mathbb{R}$ as follows.
        For the set of points $V$:
        \begin{itemize}
            \item $d(v,w)=1$ if $\{v,w\} \in E$
            \item $d(v,w)=2$ if $\{v,w\} \in \binom{V}{2} \setminus E$
            \item $d(v,v)=0$ if $v \in V$
        \end{itemize}

        \begin{claim}
            Suppose that $S$ is a set of points s.t. $Cen(S)=1$.
            Then $S$ is a dominating set of $G(V,E)$.
        \end{claim}
        \begin{proof}
            If $S$ is s.t. $Cen(S)=1$, then $\forall v \in V$, $\exists w \in S$ s.t. either $v=w$ or $\{v,w\} \in E$. Therefore, $S$ is a dominating set of $G(V,E)$.
        \end{proof}

        Thus, if $\exists$ dominating set $S$ of $k$ nodes, then $Cen(S)=1$.

        Suppose that, instead, $\not\exists S \in \binom{V}{k}$ s.t. $S$ is a dominating set of $G(V,E)$.
        Then, no matter which $k$ nodes I pick, there will be one node $v$ of the graph that is not covered by the $k$ nodes: $\forall S \in \binom{V}{k}$, $\exists v \in V \setminus S$ s.t. $\forall w \in S$ : $\{v,w\} \not\in E$.
        Then, $d(v,S) = 2$.

        Thus, the inapproximability has been proved.
        We only need to prove that $d$ is a metric.

        Let $\{v,w,x\} \in \binom{V}{3}$.
        The triangle inequality requires that $d(v,w) + d(w,x) \geq d(v,x)$.
        We have that
        \[ d(v,w) + d(w,x) \geq 2 \geq d(v,x) \]
    \end{proof}

    Without the triangle inequality, we could have set
    \[ d(v,w) = T \gg 2 \]
    for $\{ v,w \} \in \binom{V}{2} \setminus E$.
    The inapproximability ratio would have been $\geq T$.


\subsection{K-Median}
    Given a solution $S$ for K-Median, we define the cost of $S$ as
    \[ Med(S) = \sum_{v \in V} \min_{c \in S} d(v,c) \]

    We define the problem as follows.
    Given as input a metric $d$, $V$ and $k$.
    We want to find $S \in \binom{V}{k}$ that (approximately) minimizes $Med(S)$.

    \begin{observation}
        K-Median admits a $(2n)$-approximation, where $n = \cardinality{V}$.
    \end{observation}

    \begin{proof}
        $\forall S \in \binom{V}{k}$,
        \[ Med(S) = \sum_{v \in V} \min_{c \in S} d(v,c) \]
        \[ Cen(S) = \max_{v \in V} \min_{c \in S} d(v,c) \]

        Also,
        \begin{equation*}
            \begin{split}
                Cen(S) &\leq Med(S) = \sum_{v \in V} \min_{c \in S} d(v,c) \leq\\
                    &\leq \cardinality{V} \max_{v \in V} \min_{c \in S} d(v,c) = \cardinality{V} Cen(S)
            \end{split}
        \end{equation*}

        Suppose that $S'$ is a $2$-approximation for K-Center.
        Then,
        \[ Cen(S') \leq 2 \min_{S \in \binom{V}{k}} Cen(S) \]

        So,
        \[ Med(S') \leq n \cdot Cen(S') \leq 2n \min_{S \in \binom{V}{k}} Cen(S) \leq 2n \min_{S \in \binom{V}{k}} Med(S)  \]
    \end{proof}

    Let us now see an algorithm for approximating K-Median, presented in Algorithm~\ref{alg:local_search}.

    \input{algorithms/local_search.tex}

    Questions:
    \begin{enumerate}
        \item For how many iterations does the algorithm run?
        \item Once I get to a local minimum, how close will it be to a global minimum?
    \end{enumerate}

    Let $L = \{ l_1, \dots, l_k \}$ be the solution returned by \textsc{LocalSearch}.

    Let $Q = \{q_1, \dots, q_k\}$ be an optimal solution.

    Let $\pi : Q \rightarrow L$ be a function that maps each optimal center to one of its closest \textsc{LocalSearch} center.

    Let $D_j^* = \min_{q \in Q} d(j,q)$, $\forall j \in V$. $D_j^*$ is the cost paid by the optimal solution for point $j$.

    Let $D_j = \min_{l \in L} d(j,l)$, $\forall j \in V$. $D_j$ is the cost paid by the LS solution, for point $j$.

    Let $L_1, L_2, \dots, L_k$ be the partition of $V$ induced by the centers of $L$.
    That is, $\forall l \in L$, $j \in L_i \rightarrow d(j,l_i) \leq d(j,l)$.

    Let $Q_1, Q_2, \dots, Q_k$ be the partition of $V$ induced be the centers of $Q$.
    That is, $\forall q \in Q$, $j \in Q_i \rightarrow d(j,q_i) \leq d(j,q)$.

    \begin{equation*}
        \begin{split}
            &\textsc{OPT} = \sum_{j \in V} D_j^* = \sum_{j \in V} \min_{q \in Q} d(j,q) = \sum_{i=1}^{k} \sum_{j \in Q_i} d(j,q_i)\\
            &\textsc{ALG} = \sum_{j \in V} D_j = \sum_{j \in V} \min_{l \in L} d(j,l) = \sum_{i=1}^{k} \sum_{j \in L_i} d(j,l_i)
        \end{split}
    \end{equation*}

    Our goal is to bound $\frac{\textsc{ALG}}{\textsc{OPT}}$.

    We first consider the special case where $\pi$ is a bijection.
    If $\pi$ is a bijection, we can assume, wlog, that $\pi(q_i) = l_i$, $\forall i \in \{ 1, \dots, k \}$.

    \begin{lemma}\label{lemma:kmedian}
        Let $L$ be the set of points returned by LS.
        Then, $\forall i \in [k]$,
        \[ 0 \leq Med(L \setminus \{l_i\} \cup \{q_i\}) - Med(L) \leq \sum_{j \in Q_i}(D_j^* - D_j) + 2 \sum_{j \in L_i} D_j^* \]
    \end{lemma}

    \begin{proof}
        Let us define $L' = L \setminus \{l_i\} \cup \{q_i\}$.

        By the LS stopping condition, $Med(L') \geq Med(L)$, thus $Med(L') - Med(L) \geq 0$.

        So,
        \[ Med(L') = \sum_{j \in V} d(j,L') = \sum_{j \in V} \min_{l \in L'} d(j,l) \]

        We now bound $\sum_{j \in V} d(j,L')$, term-by-term.
        Consider three cases:
        \begin{enumerate}
            \item $j \in Q_i$
            \item $j \in L_i \setminus Q_i$
            \item $j \in V \setminus (L_i \cup Q_i)$
        \end{enumerate}

        Case 1. If $j \in Q_i$, we could serve $j$ with $q_i \in L'$. Thus,
        \begin{equation*}
            d(j,L') = \min_{q \in L'} d(j,q) \overset{q_i \in L'}{\leq} d(j,q_i) = D_j^*
        \end{equation*}

        Case 2. If $j \in L_i \setminus Q_i$, then $j$ is not served by $q_i$ in the optimal solution we chose.
        Say that the optimal solution serves $j$ with $q_r \in Q$.
        Then, $q_r \neq q_i$. Also, recall that $\pi(q_r) = l_r \neq l_i$, since we have assumed that $\pi$ is a bijection. Thus $\pi(q_r) \in L'$.
        \begin{equation*}
            \begin{split}
                d(j,L') &\overset{\pi(q_r)\in L'}{\leq} d(j, \pi(q_r)) =\\
                    &= d(j,l_r) \overset{\text{tr. ineq.}}{\leq}\\
                    &\leq d(j,q_r) + d(q_r, l_r) =\\
                    &= D_j^* + d(q_r, l_r) \overset{\substack{\pi(q_r) = lr\\ \text{the closest center}}}{\leq}\\
                    &\leq D_j^* + d(q_r, l_i) \leq\\
                    &\leq D_j^* + d(q_r, j) + d(j, l_i) =\\
                    &= D_j^* + D_j^* + D_j =\\
                    &= 2D_j^* + D_j
            \end{split}
        \end{equation*}

        Case 3. If $j \in V \setminus (L_i \cup Q_i)$, then $j$ can be served by its closest center in the LS solution, thus
        \begin{equation*}
            d(j,L') \leq D_j
        \end{equation*}

        Thus,
        \begin{equation*}
            \begin{split}
                Med(L') &= \sum_{j \in V} d(j,L') =\\
                    &= \sum_{j \in Q_i} d(j,L') + \sum_{j \in L_i \setminus Q_i} d(j,L') + \sum_{j \in V \setminus (L_i \cup Q_i)} d(j,L') \leq\\
                    &\leq \sum_{j \in Q_i} D_j^* + \sum_{j \in L_i \setminus Q_i} (2 D_j^* + D_j) + \sum_{j \in V \setminus (L_i \cup Q_i)} D_j
            \end{split}
        \end{equation*}

        And
        \[ Med(L) = \sum_{j \in V} D_j \]

        \begin{equation*}
            \begin{split}
                Med(L') - Med(L) &\leq \sum_{j \in Q_i} (D_j^* - D_j) + \sum_{j \in L_i \setminus Q_i} (2 D_j^* + D_j - D_j) + \sum_{j \in V \setminus (L_i \cup Q_i)} (D_j - D_j) =\\
                    &= \sum_{j \in Q_i} (D_j^* - D_j) + 2 \sum_{j \in L_i \setminus Q_i} D_j^* \leq\\
                    &\leq \sum_{j \in Q_i} (D_j^* - D_j) + 2 \sum_{j \in L_i} D_j^*
            \end{split}
        \end{equation*}
    \end{proof}

    \begin{theorem}
        If $\pi$ is a bijection, the LS solutions is a $3$-approximation to K-Median.
    \end{theorem}

    \begin{proof}
        We use Lemma~\ref{lemma:kmedian} to sum up the $k$ bounds ($i = 1, 2, \dots, k$):
        \begin{equation*}
            \begin{split}
                0 &\leq \sum_{i=1}^{k} Med(L \setminus \{l_i\} \cup \{q_i\}) - k \cdot Med(L) \leq\\
                    &\leq \sum_{i=1}^{k} \sum_{j \in Q_i} (D_j^* - D_j) + 2 \sum_{i=1}^{k} \sum_{j \in L_i} D_j^* =\\
                    &= \sum_{j \in V} (D_j^* - D_j) + 2 \sum_{j \in V} D_j^* =\\
                    &= 3 \sum_{j \in V} D_j^* - \sum_{j \in V} D_j =\\
                    &= 3 \textsc{OPT} - \textsc{ALG}
            \end{split}
        \end{equation*}

        Then, $\textsc{ALG} \leq 3 \textsc{OPT}$.
    \end{proof}

    The case just studied, where $\pi$ is a bijection, although quite common in real life, is not true in general.
    Let us now consider the case in which $\pi$ is not a bijection; that is, where multiple $q_i$'s get mapped to the same $l_j$.

    Note that now:
    \begin{itemize}
        \item some $q_i$'s end up being mapped to the same $l_j$
        \item some $q_i$ are still mapped one-to-one to some $l_j$ (as in the bijective case)
        \item some $l_i$'s end up not being mapped at all
    \end{itemize}

    Let $T \subseteq L$ be defined as
    \[ T = \{ l \st l \in L \wedge \exists \{q,q'\} \in \binom{Q}{2} : \pi(q) = \pi(q') = l \} \]

    Let $Z \subseteq L$ be defined as
    \[ Z = \{ l \st l \in L \wedge \not\exists q \in Q : \pi(q) = l \} \]

    Let $M \subseteq Q$ be defined as
    \[ M = \{ q \st q \in Q \wedge \exists q' \in Q \setminus \{q\} : \pi(q) = \pi(q') \} \]

    \begin{lemma}
        $2 \cardinality{Z} \geq \cardinality{M}$
    \end{lemma}

    \begin{proof}
        We have that $\cardinality{Z} + \cardinality{T} = \cardinality{M}$.
        Because $\cardinality{L} = \cardinality{Q} = k$, and there are as many $1$-matched $q$'s as there are $1$-matched $l$'s, so the only points that remain are the ones in $Z, T$ and $M$.

        Moreover, the number of "arcs" between points fo $M \cup Z \cup T$ is equal to $M$.

        Also, the number of "arcs" is at least $2 \cardinality{T}$, since each node in $T$ is hit by at least $2$ arcs.

        \begin{equation*}
            \begin{split}
                &\cardinality{T} = \cardinality{M} - \cardinality{Z}\\
                &2 (\cardinality{M} - \cardinality{Z}) = 2 \cardinality{T} \leq (\text{\# of arcs in } M \cup Z \cup T) = \cardinality{M}\\
                &2 \cardinality{M} - 2 \cardinality{Z} \leq \cardinality{M}\\
                &2 \cardinality{Z} \geq \cardinality{M}
            \end{split}
        \end{equation*}
    \end{proof}

    Now, let us define an $L,Q$-match as a pair $(l,q)$ s.t. $l \in L$ and $q \in Q$.
    We will consider the following set of $L,Q$ matches.

    If $\pi(q)$ is unique to $q$ (if $q \in Q \setminus M$), then we add the match $(\pi(q), q)$.

    For each of the remaining $q$'s, we act as follows.
    We match $q \in M$ to elements of $Z$ in such a way that:
    \begin{itemize}
        \item each $q \in M$ is part of exactly 1 match
        \item each $l \in Z$ is part of exactly 0, 1 or 2 matches
    \end{itemize}

    These matches can be created, since $\cardinality{M} \leq 2 \cardinality{Z}$.
    Thus, if you have some extra $q \in M$ to match, just match it to some $l \in Z$ that has been matches the fewest times.

    Let $P$ be the set of matches created by this algorithm.
    Note that we don't really have to come up with $P$ when we run the LS, we just consider $P$ for analysis' sake.

    \begin{lemma}\label{lemma:kmedian_lqmatches}
        $\forall (l_y, q_x) \in P$,
        \[ 0 \leq Med(L \setminus \{l_y\} \cup \{q_x\}) - Med(L) \leq \sum_{j \in Q_x} D_j^* - \sum_{j \in Q_x} D_j + 2 \sum_{j \in L_y} D_j^* \]
    \end{lemma}

    \begin{exercise}
        Prove Lemma~\ref{lemma:kmedian_lqmatches}.
    \end{exercise}

    \begin{theorem}
        In the general where (where $\pi$ is not a bijection), LS provides a $5$-approximation.
    \end{theorem}

    \begin{proof}
        We sum up the $k$ inequalities, one for each $(l_y, q_x) \in P$.
        \begin{equation*}
            \begin{split}
                0 &\leq \sum_{(l_y, q_x) \in P} (Med(L \setminus \{l_y\} \cup \{q_x\}) - Med(L)) \leq\\
                &\leq \sum_{(l_y,q_x) \in P} \sum_{j \in Q_x} D_j^* - \sum_{(l_y,q_x) \in P} \sum_{j \in Q_x} D_j + 2 \sum_{(l_y,q_x) \in P} \sum_{j \in L_y} D_j^* \overset{\substack{\text{each } q_x \in Q\\\text{ appears in exactly 1 match}}}{=}\\
                &= \sum_{j \in V} D_j^* - \sum_{j \in V} D_j + 2 \sum_{(l_y,q_x) \in P} \sum_{j \in L_y} D_j^* \overset{\substack{\text{each } l_y \in L\\ \text{ appears in at most 2 matches}}}{\leq}\\
                &\leq 5 \textsc{OPT} = 5 \sum_{j \in V} D_j^* \geq \sum_{j \in V} D_j = \textsc{ALG}
            \end{split}
        \end{equation*}
    \end{proof}

    A couple of observations on the running time of Algorithm~\ref{alg:local_search}:
    \begin{itemize}
        \item If, at the outset (i.e.~beginning), $S$ is a local optimum, then the algorithm and the analysis provides a short (i.e.~polynomial-size) proof that $S$ is a $5$-approximation
        \item In practive, this LS converges pretty quickly, after 10s of iterations
    \end{itemize}

    \input{algorithms/local_search_epsilon.tex}

    \begin{theorem}
        $LS_\varepsilon$ provides a $(5+\varepsilon)$-approximation to K-Median.
    \end{theorem}

    \begin{proof}
        \begin{equation*}
            \begin{split}
                \sum_{(l_y,q_x) \in P} (-\varepsilon \frac{Med(S_0)}{2 n k}) &\leq \sum_{(l_y, q_x) \in P} (Med(L \setminus \{l_y\} \cup \{q_x\}) - Med(L)) \leq\\
                    & \leq 5 \textsc{OPT} - \textsc{ALG}
            \end{split}
        \end{equation*}

        \begin{equation*}
            \begin{split}
                &5 \textsc{OPT} - \textsc{ALG} \geq \cardinality{P} (- \varepsilon \frac{Med(S_0)}{2 n k})\\
                &5 \textsc{OPT} - \textsc{ALG} \geq - \varepsilon k \frac{Med(S_0)}{2 n k}
            \end{split}
        \end{equation*}

        \begin{equation*}
            \begin{split}
                \textsc{ALG} &\leq 5 \textsc{OPT} + \varepsilon \frac{Med(S_0)}{2n} \leq 5 \textsc{OPT} + \varepsilon \frac{2 n \textsc{OPT}}{2 n} = (5 + \varepsilon) \textsc{OPT}
            \end{split}
        \end{equation*}
    \end{proof}

    \begin{theorem}
        $LS_\varepsilon$ runs for at most $O(\frac{n k}{\varepsilon})$ iterations (thus, in polytime).
    \end{theorem}

    \begin{proof}
        Let $S_i$ be the value of $S$ when the $i$-th iteration ends.

        Suppose that the loop runs for $\ell$ iterations.
        $\forall i = 0, \dots, \ell - 1$, the following holds
        \[ Med(S_{i+1}) \leq Med(S_i) - \varepsilon \frac{Med(S_0)}{2 n k} \]

        Then,
        \begin{equation*}
            \begin{split}
                &Med(S_1) \leq Med(S_0) - \varepsilon \frac{Med(S_0)}{2 n k} = (1 - \frac{\varepsilon}{2 n k}) Med(S_0)\\
                &Med(S_2) \leq Med(S_1) - \varepsilon \frac{Med(S_0)}{2 n k} \leq (1 - \frac{2 \varepsilon}{2 n k}) Med(S_0)\\
                \vdots\\
                &Med(S_i) \leq (1 - \frac{i \varepsilon}{2 n k}) Med(S_0)\\
                \vdots
            \end{split}
        \end{equation*}

        Then,
        \[ 0 \leq Med(S_\ell) \leq (1 - \frac{\ell \varepsilon}{2 n k}) Med(S_0) \]

        Thus, given that $Med(S_0) > 0$, it must hold that $1 - \frac{\ell \varepsilon}{2 n k} \geq 0$.
        
        Then $\ell \leq \frac{2 n k}{\varepsilon}$.
    \end{proof}